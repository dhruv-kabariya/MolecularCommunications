# Bayesian regularization back propagation technique
# hyperbolic tangent sig-moid activation function == tanh fn

# he number of hidden layers is 10 == 10
# the numberof neurons per layer is 5 == 5
# the learning rate is 0.01 = alpha = 0.01
#  the trainingepoch is 200 = 200
# the total number of training bits is 50000 == 500 dataset
# andthe number of test bits is 100000 == 100000 test bit
# batch mode, and the number of bits in eachbatch is 1000 == 1000 bit
